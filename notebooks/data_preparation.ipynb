{"cells":[{"cell_type":"markdown","source":["## Data preparation\n\nWe apply the following transformation to the input text data:\n\n+ Clean strings\n+ Tokenize (`String -> Array<String>`)\n+ Remove stop words\n+ Stem words\n+ Create bigrams"],"metadata":{}},{"cell_type":"markdown","source":["#### 0. Create DataFrame"],"metadata":{}},{"cell_type":"code","source":["# Set table name\ntable_name = \"faam_dataset_v4\"\n\n# Create DF from table\ntweet_df = sqlContext.table(table_name)\n\n# Random sampling (20%)\ntweet_df = tweet_df.sample(False, 0.2)\n\n# Display schema and data sample\ntweet_df.printSchema()\nprint(\"Size of the DataFrame: {} records\".format(tweet_df.count()))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["#### 1. Clean text string"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, lower, regexp_replace, split\n\ndef clean_text(c):\n  c = lower(c)\n  c = regexp_replace(c, \"^rt \", \"\")\n  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n  c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n  #c = split(c, \"\\\\s+\") tokenization...\n  return c\n\nclean_text_df = tweet_df.select(clean_text(col(\"text\")).alias(\"text\"))\n\nclean_text_df.printSchema()\nclean_text_df.show(10)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["#### 2. Tokenize"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"vector\")\nvector_df = tokenizer.transform(clean_text_df).select(\"vector\")\n\nvector_df.printSchema()\nvector_df.show(10)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### 3. Remove stop words"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\n\n# Define a list of stop words or use default list\nremover = StopWordsRemover()\nstopwords = remover.getStopWords() \n\n# Display default list\nstopwords[:10]"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Specify input/output columns\nremover.setInputCol(\"vector\")\nremover.setOutputCol(\"vector_no_stopw\")\n\n# Transform existing dataframe with the StopWordsRemover\nvector_no_stopw_df = remover.transform(vector_df).select(\"vector_no_stopw\")\n\n# Display\nvector_no_stopw_df.printSchema()\nvector_no_stopw_df.show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### 4. Stem tokens"],"metadata":{}},{"cell_type":"code","source":["# Import stemmer library\nfrom nltk.stem.porter import *\n\n# Instantiate stemmer object\nstemmer = PorterStemmer()\n\n# Quick test of the stemming function\ntokens = [\"thanks\", \"its\", \"proverbially\", \"unexpected\", \"running\"]\nfor t in tokens:\n  print(stemmer.stem(t))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Create stemmer python function\ndef stem(in_vec):\n    out_vec = []\n    for t in in_vec:\n        t_stem = stemmer.stem(t)\n        if len(t_stem) > 2:\n            out_vec.append(t_stem)       \n    return out_vec\n\n# Create user defined function for stemming with return type Array<String>\nfrom pyspark.sql.types import *\nstemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n\n# Create new df with vectors containing the stemmed tokens \nvector_stemmed_df = (\n    vector_no_stopw_df\n        .withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n        .select(\"vector_stemmed\")\n  )\n\n# Rename df and column for clarity\nproduction_df = vector_stemmed_df.select(col(\"vector_stemmed\").alias(\"unigrams\"))\n\n# Display\nproduction_df.printSchema()\nproduction_df.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["#### 5. Create bigrams"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import NGram\n\n# Define NGram transformer\nngram = NGram(n=2, inputCol=\"unigrams\", outputCol=\"bigrams\")\n\n# Create bigram_df as a transform of unigram_df using NGram tranformer\nproduction_df = ngram.transform(production_df)\n\n# Display\nproduction_df.printSchema()\nproduction_df.show()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["#### 6. Filter out small/empty vectors"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, size\n\nproduction_df = production_df.where(size(col(\"bigrams\")) >= 2)\n\n# Display\nproduction_df.printSchema()\nproduction_df.show()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Save production data as Spark table for analysis"],"metadata":{}},{"cell_type":"code","source":["production_df.write.saveAsTable(\"faam_dataset_production\")"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"data_preparation","notebookId":1357850364289680},"nbformat":4,"nbformat_minor":0}
