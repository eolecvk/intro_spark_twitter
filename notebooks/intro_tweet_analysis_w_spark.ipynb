{"cells":[{"cell_type":"markdown","source":["# Introduction to text analysis with PySpark using Twitter data\n\nObjective:\n_Using the Spark Python API (PySpark) in a Databricks notebook environment to perform some basic text mining._\n\n***************************************************************************************\n\n**PLAN**\n\n\n1. Data overview\n    + Tweet object\n    + JSON format\n    + Spark DataFrame\n    + Databricks table\n    \n2. Data sourcing\n    + (prior) Get Twitter data using the REST API\n    + (prior) Aggregate tweet collection as a single JSON file\n    + (prior) Upload the JSON source file to a Databricks table \n    + Create dataframe from Databricks table\n    \n3. Data exploration\n   + Show dataframe, print schema\n   + Basic sql queries\n   + User tweet frequency bar graph\n   + Count tweets containing a given keyword\n\n4. Text preprocessing\n   + tokenization (unigram, bigram, ...)\n   + stop word removal\n   + lemmatization (stemming, synonym expansion)\n\n5. Text analysis\n   + Clustering?\n\n***************************************************************************************\n**Doc & programming guides**\n+ [PySpark doc](https://spark.apache.org/docs/latest/api/python/)\n+ [Spark SQL programming guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n+ [Databricks doc](https://docs.databricks.com/)\n+ [Twitter API overview](https://dev.twitter.com/overview/api)\n\n**Tutorials**\n+ [Databricks workshop doc](http://training.databricks.com/workshop/sparkcamp.pdf)\n+ [Scala crash course](https://lintool.github.io/SparkTutorial/slides/day1_Scala_crash_course.pdf)\n\n***************************************************************************************\n\n\n***************************************************************************************\n\n## 1. Overview of the relevant data objects and structures\n\n#### [Tweets](https://dev.twitter.com/overview/api/tweets)\n\n+ Relevant fields considered for this presentation: `tweet_id`, `user_id` and `text`\n+ Natively in `.json` format\n+ cf tweet sample\n\n#### JSON format\n\n+ Semi-structured data format\n+ [Databricks specific JSON format requirements](https://docs.databricks.com/spark/latest/data-sources/read-json.html)\n+ Python script to extract a collection of `.json` tweet files to a single `.json` file.\n\n\n#### [Spark Dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes)\n\n+ A DataFrame is a distributed collection of data organized into named columns\n+ Supports (domain-specific) DataFrame Operations and SQL queries\n\n\n#### [Spark table](https://docs.databricks.com/user-guide/tables.html)\n\n+ Tables are a simple way to make structured data available across an organization\n+ Tables are essentially Spark dataframe that are persisted in the Hive metastore (data warehouse)\n+ A user can query tables with Spark SQL or any of Apache Sparkâ€™s language APIs\n+ In Databricks, tables can be created from local files using the Data Import UI (_> table > Create Table > Local file_)\n+ In Databricks, supported source file formats for creating tables include `.json`, `.csv`, `.avro` and `.parquet` \n\n\n***************************************************************************************\n\n## 2. Data sourcing\n\n### Priors\n\n+ Get Twitter data using the REST API\n+ Aggregate tweet collection as a single JSON file\n+ Upload the JSON source file to a Databricks table \n\n### Create dataframe from Databricks table"],"metadata":{}},{"cell_type":"code","source":["# Set table name\ntable_name = \"faam_dataset_v4\"\n\n# Option_1: Using SQL query\ntweet_df_sql = sql(\"SELECT * FROM {}\".format(table_name))\n\n# Option_2: Using sqlContext\ntweet_df_sqlContext = sqlContext.table(table_name)\n\n# Random sampling\n#tweet_df = tweet_df.sample(False, 0.2)\n\n# Display schema and data sample\ntweet_df.printSchema()\ntweet_df.show(10)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["-- quick exploration: count entities"],"metadata":{}},{"cell_type":"code","source":["# Quick df facts\ndf_size = tweet_df.count()\nnum_unique_tweets = tweet_df.select('tweet_id').distinct().count()\nnum_unique_users = tweet_df.select('user_id').distinct().count()\n\nprint(\"Number of records: {}\".format(df_size))\nprint(\"Number of unique tweets: {}\".format(num_unique_tweets))\nprint(\"Number of unique users: {}\".format(num_unique_users))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["-- User tweet frequency distribution"],"metadata":{}},{"cell_type":"code","source":["# User tweet frequency distribution\nuser_tweet_count_df = tweet_df.groupBy(\"user_id\").count()\n\n# Retrieve max user tweet frequency\nmax_count = user_tweet_count_df.agg({\"count\": \"max\"}).collect()[0][0]\n\n\n# Iterate through frequency ranges from 0 to max_count\nstep = 200\nfor threshold in range(0, max_count, step):\n    \n    # Determine freq of users with a tweet count in range(threshold, threshold+step)\n    step_freq = user_tweet_count_df\\\n    .filter(\"count>{}\".format(threshold))\\\n    .filter(\"count<{}\".format(threshold+step))\\\n    .count()\n    \n    # Display bar the given tweet count bucket\n    print(\"{threshold}{indent}{freqdots}\".format(\n      threshold=threshold,\n      indent=' ' * (5-len(str(threshold))),\n      freqdots='.' * step_freq))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### How many tweets contain a given keyword?\n\n#### Define regex for detecting the keyword"],"metadata":{}},{"cell_type":"code","source":["import re\n\n# Define regular expression\nsubstr = 'lol'\nregex = \"(?<![a-z])({substring})(?![a-z])\".format(substring=substring)\n\n# Test regular expression\ntest_values = [\n  (\"lollipop\", False),\n  (\"lol! \", True),\n  (\"...lol\", True),\n  (\"lmao lol haha\", True),\n  (\"/LoL/\", True)\n  ]\n\ntest_results = []\nfor test_val in test_values:\n    test_result = (re.search(regex, test_val[0].lower()) is not None) ==  test_val[1]\n    test_results.append(test_result)\n\nif all(test_results):\n    print(\"SUCCESS: the regex passed ALL the tests\")\nelse:\n    failed_test = ', '.join([str(test_index+1) for test_index, test_res in list(enumerate(test_results)) if not test_res])\n    print(\"FAILED test# {}\".format(failed_test))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#### Option1: Use an SQL query"],"metadata":{}},{"cell_type":"code","source":["# Register the DataFrame as a SQL temporary view\ntweet_df.createOrReplaceTempView(\"tweets\")\n\n# Query temp view using SQL syntax\nsql(\"SELECT * FROM tweets WHERE LOWER(text) RLIKE '{}'\".format(regex)).count()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Option#2: Use RDD `filter()`"],"metadata":{}},{"cell_type":"code","source":["import re\n\n(\ntweet_df\n    .select(\"text\")\n    .rdd\n    .map(lambda x: unicode(x[0])) # convert row type to string type\n    .filter(lambda x: re.search(regex, x.lower()) is not None) # filter records using a custom lambda function\n    .count()\n)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Data preparation\n\nWe apply the following transformation to the input text data:\n\n+ Clean strings\n+ Tokenize (`String -> Array<String>`)\n+ Remove stop words\n+ Stem words\n+ Create bigrams\n\n\n#### 1. Clean text string"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, lower, regexp_replace, split\n\ndef clean_text(c):\n  c = lower(c)\n  c = regexp_replace(c, \"^rt \", \"\")\n  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n  c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n  #c = split(c, \"\\\\s+\") tokenization...\n  return c\n\nclean_text_df = tweet_df.select(clean_text(col(\"text\")).alias(\"text\"))\n\nclean_text_df.printSchema()\nclean_text_df.show(10)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#### 2. Tokenize"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"vector\")\nvector_df = tokenizer.transform(clean_text_df).select(\"vector\")\n\nvector_df.printSchema()\nvector_df.show(10)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["#### 3. Remove stop words"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\n\n# Define a list of stop words or use default list\nremover = StopWordsRemover()\nstopwords = remover.getStopWords() \n\n# Display default list\nstopwords[:10]"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Specify input/output columns\nremover.setInputCol(\"vector\")\nremover.setOutputCol(\"vector_no_stopw\")\n\n# Transform existing dataframe with the StopWordsRemover\nvector_no_stopw_df = remover.transform(vector_df).select(\"vector_no_stopw\")\n\n# Display\nvector_no_stopw_df.printSchema()\nvector_no_stopw_df.show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["#### 5. Stem tokens"],"metadata":{}},{"cell_type":"code","source":["# Import stemmer library\nfrom nltk.stem.porter import *\n\n# Instantiate stemmer object\nstemmer = PorterStemmer()\n\n# Quick test of the stemming function\ntokens = [\"thanks\", \"its\", \"proverbially\", \"unexpected\", \"running\"]\nfor t in tokens:\n  print(stemmer.stem(t))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# Create stemmer python function\ndef stem(in_vec):\n    out_vec = []\n    for t in in_vec:\n        t_stem = stemmer.stem(t)\n        if len(t_stem) > 2:\n            out_vec.append(t_stem)       \n    return out_vec\n\n# Create user defined function for stemming with return type Array<String>\nfrom pyspark.sql.types import *\nstemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n\n# Create new df with vectors containing the stemmed tokens \nvector_stemmed_df = (\n    vector_no_stopw_df\n        .withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n        .select(\"vector_stemmed\")\n  )\n\n# Display\nvector_stemmed_df.printSchema()\nvector_stemmed_df.show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### 6. Create bigrams"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import NGram\n\n# Define NGram transformer\nngram = NGram(n=2, inputCol=\"vector_stemmed\", outputCol=\"bigrams\")\n\n# Create bigram_df as a transform of unigram_df using NGram tranformer\nbigrams_df = ngram.transform(vector_filtered_df)\n\n# Display\nbigrams_df.printSchema()\nbigrams_df.show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### 7. Filter out small/empty vectors"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, size\n\nproduction_df = bigrams_df.where(size(col(\"bigrams\")) >= 2)\n\n# Display\nproduction_df.printSchema()\nproduction_df.show()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"intro_tweet_analysis_w_spark","notebookId":1357850364289680},"nbformat":4,"nbformat_minor":0}
