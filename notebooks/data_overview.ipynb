{"cells":[{"cell_type":"markdown","source":["## Overview of the relevant data objects and structures\n\n#### Tweet objects [[doc](https://dev.twitter.com/overview/api/tweets)]\n\n+ Relevant fields considered for this presentation: `tweet_id`, `user_id` and `text`\n+ Natively in `.json` format\n+ See [tweet sample](https://github.com/eolecvk/intro_spark_twitter/blob/master/data/tweet_sample.json) for reference\n\n#### JSON format [[doc](http://www.json.org/)]\n\n+ Semi-structured data format\n+ [Databricks specific JSON format requirements](https://docs.databricks.com/spark/latest/data-sources/read-json.html)\n+ Python script to extract a collection of `.json` tweet files to a single `.json` file\n\n#### Spark Dataframes [[doc](https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes)]\n\n+ A DataFrame is a distributed collection of data organized into named columns\n+ Supports SQL queries in addition to (domain-specific) DataFrame Operations\n\n\n#### Spark table [[doc](https://docs.databricks.com/user-guide/tables.html)]\n\n+ Tables are a simple way to make structured data available across an organization\n+ Tables are essentially Spark dataframe that are persisted in the Hive metastore (data warehouse)\n+ A user can query tables with Spark SQL or any of Apache Sparkâ€™s language APIs\n+ In Databricks, tables can be created from local files using the Data Import UI (_> table > Create Table > Local file_)\n+ In Databricks, supported file formats for creating tables from the UI include `.json`, `.csv`, `.avro` and `.parquet`"],"metadata":{}}],"metadata":{"name":"data_overview","notebookId":3963252404083091},"nbformat":4,"nbformat_minor":0}
